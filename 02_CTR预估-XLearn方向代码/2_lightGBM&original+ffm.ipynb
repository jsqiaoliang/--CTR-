{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先 import 必要的模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "#from lgbm.sklearn import LGBMClassifier\n",
    "import xlearn as xl\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss     #采用logloss作为评价指标\n",
    "from pandas import DataFrame\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from libffm_data_generate import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、 数据输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_name_1='100_0'\n",
    "train_name_2='100_1'\n",
    "valid_name_1='100_2'\n",
    "valid_name_2='100_3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(999936, 25)\n",
      "(999188, 25)\n"
     ]
    }
   ],
   "source": [
    "#train_data\n",
    "train_1 = pd.read_csv(\"/home/jupyter/data_final_29/100w/\"+train_name_1,index_col=0).reset_index(drop=True)   #拿200万条数据训练\n",
    "train_1.drop(['id','hour','user_id','hour_days'],axis=1,inplace=True)\n",
    "print(train_1.shape)\n",
    "#validation_data\n",
    "valid_1 = pd.read_csv(\"/home/jupyter/data_final_29/100w/\"+valid_name_1,index_col=0).reset_index(drop=True)   #拿200万条数据训练\n",
    "valid_1.drop(['id','hour','user_id','hour_days'],axis=1,inplace=True)\n",
    "print(valid_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(999936, 25)\n",
      "(994263, 25)\n"
     ]
    }
   ],
   "source": [
    "#train_data\n",
    "train_2 = pd.read_csv(\"/home/jupyter/data_final_29/100w/\"+train_name_2,index_col=0).reset_index(drop=True)   #拿200万条数据训练\n",
    "train_2.drop(['id','hour','user_id','hour_days'],axis=1,inplace=True)\n",
    "print(train_2.shape)\n",
    "valid_2 = pd.read_csv(\"/home/jupyter/data_final_29/100w/\"+valid_name_2,index_col=0).reset_index(drop=True)   #拿200万条数据训练\n",
    "valid_2.drop(['id','hour','user_id','hour_days'],axis=1,inplace=True)\n",
    "print(valid_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4218938, 25)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test_data\n",
    "test = pd.read_csv(\"/home/jupyter/data_final_29/test\",index_col=0).reset_index(drop=True)   #拿200万条数据训练\n",
    "test.drop(['id','hour','user_id','hour_days'],axis=1,inplace=True)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first\n",
    "y_train_1 = train_1['click']\n",
    "x_gbm_train_1=train_1[['user_hour_C14','user_hour_C17','user_hour_app_id']]\n",
    "x_ffm_train_1=train_1.drop(['user_hour_C14','user_hour_C17','user_hour_app_id',\"click\"], axis=1)\n",
    "\n",
    "y_valid_1 = valid_1['click']\n",
    "x_gbm_valid_1=valid_1[['user_hour_C14','user_hour_C17','user_hour_app_id']]\n",
    "x_ffm_valid_1=valid_1.drop(['user_hour_C14','user_hour_C17','user_hour_app_id',\"click\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second\n",
    "y_train_2 = train_2['click']\n",
    "x_gbm_train_2=train_2[['user_hour_C14','user_hour_C17','user_hour_app_id']]\n",
    "x_ffm_train_2=train_2.drop(['user_hour_C14','user_hour_C17','user_hour_app_id',\"click\"], axis=1)\n",
    "\n",
    "y_valid_2 = valid_2['click']\n",
    "x_gbm_valid_2=valid_2[['user_hour_C14','user_hour_C17','user_hour_app_id']]\n",
    "x_ffm_valid_2=valid_2.drop(['user_hour_C14','user_hour_C17','user_hour_app_id',\"click\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test['click']\n",
    "x_gbm_test=test[['user_hour_C14','user_hour_C17','user_hour_app_id']]\n",
    "x_ffm_test=test.drop(['user_hour_C14','user_hour_C17','user_hour_app_id',\"click\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_1\n",
    "del train_2\n",
    "del valid_1\n",
    "del valid_2\n",
    "del test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 将连续性特征输入lightgbm生成编码后的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qiaoliang/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:102: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.481795072555542\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "# 定义模型\n",
    "lgb1 = lgbm.LGBMRegressor(\n",
    "          num_boost_round=20,\n",
    "          boosting='gbdt', \n",
    "          objective='binary', \n",
    "          num_threads=6, \n",
    "          #default=OpenMP_default, type=int, alias=num_thread, nthread LightGBM 的线程数\n",
    "#          'silent': True,\n",
    "          learning_rate=0.1, \n",
    "          num_leaves=10, \n",
    "          #num_leaves：叶子数\n",
    "#num_leaves, default=31, type=int, alias=num_leaf一棵树上的叶子数\n",
    "          max_depth= 3,\n",
    "          max_bi= 256, \n",
    "          subsample_for_bin= 200000,#默认200000，用来构建直方图数据数量\n",
    "          subsample= 0.8, #默认1\n",
    "          #类似于feature_fraction,它将在不进行重采样的情况下\n",
    "          #随机选择部分数据可以用来加速训练可以用来处理过拟合Not\n",
    "          subsample_freq= 1, \n",
    "          colsample_bytree= 0.8,       \n",
    "          reg_alpha= 0.8,         \n",
    "          reg_lambda= 0.25, \n",
    "          min_split_gain=0.0, \n",
    "          min_child_weight= 1,        \n",
    "          min_child_samples= 200, \n",
    "          min_data_in_leaf=50, \n",
    "          #default=20, type=int, alias=min_data_per_leaf , min_data, \n",
    "          #min_child_samples一个叶子上数据的最小数量. 可以用来处理过拟合.\n",
    "        \n",
    "        # 'categorical_feature':3\n",
    "         # 'categorical_feature':3:26\n",
    "         )\n",
    "# 训练学习\n",
    "model=lgb1.fit(x_gbm_train_1, y_train_1)\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss of train : 0.44975218594366606\n",
      "logloss of valid : 0.4496632713427818\n",
      "logloss of test : 0.44907588321989766\n"
     ]
    }
   ],
   "source": [
    "# train上logloss评测\n",
    "y_pred_train = lgb1.predict(x_gbm_train_1)\n",
    "logloss = log_loss(y_train_1, y_pred_train)\n",
    "print (\"logloss of train :\",logloss )\n",
    "\n",
    "# valid上logloss评测\n",
    "y_pred_valid = lgb1.predict(x_gbm_valid_1)\n",
    "logloss = log_loss(y_valid_1, y_pred_valid)\n",
    "print (\"logloss of valid :\",logloss )\n",
    "\n",
    "# test上logloss评测\n",
    "y_pred_test = lgb1.predict(x_gbm_test)\n",
    "logloss = log_loss(y_test, y_pred_test)\n",
    "print (\"logloss of test :\",logloss )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 生成每颗树最大叶子节点的字典，需手动设置n_tree和num_leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_gbm={}\n",
    "n_tree=20\n",
    "num_leaves=10\n",
    "for i in range(n_tree):\n",
    "    dict_gbm[i]=num_leaves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 第1组数据，将经过gbm处理的连续特征按叶子节点输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(999936, 20)\n",
      "(999188, 20)\n",
      "(4218938, 20)\n"
     ]
    }
   ],
   "source": [
    "x_train_leaves = lgb1.predict(x_gbm_train_1, pred_leaf=True)    \n",
    "x_valid_leaves = lgb1.predict(x_gbm_valid_1, pred_leaf=True) \n",
    "x_test_leaves = lgb1.predict(x_gbm_test, pred_leaf=True) \n",
    "print(x_train_leaves.shape)\n",
    "print(x_valid_leaves.shape)\n",
    "print(x_test_leaves.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 将第2组数据，将经过gbm处理的连续特征按叶子节点输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 30)\n",
      "(1000, 30)\n"
     ]
    }
   ],
   "source": [
    "x_train_leaves = lgb1.predict(x_gbm_train_2, pred_leaf=True)    \n",
    "x_valid_leaves = lgb1.predict(x_gbm_valid_2, pred_leaf=True) \n",
    "print(x_train_leaves.shape)\n",
    "print(x_valid_leaves.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 3、将原始特征与经GBM编码后的特征进行整合处理为供FFM的特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 lightgbm输出叶子节点数据转化为libffm或liffm格式数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310.03121614456177\n"
     ]
    }
   ],
   "source": [
    "#ffm-data\n",
    "#train\n",
    "start=time.time()\n",
    "original_gbm_ffm_data(x_ffm_train_1,y_train_1,dict_gbm,x_train_leaves,save_name=r'/home/jupyter/xlearn/gbm_orig_ffm/ffm_'+train_name_1+'.txt')\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293.6466438770294\n"
     ]
    }
   ],
   "source": [
    "#valid\n",
    "start=time.time()\n",
    "original_gbm_ffm_data(x_ffm_valid_1,y_valid_1,dict_gbm,x_valid_leaves,save_name=r'/home/jupyter/xlearn/gbm_orig_ffm/ffm_'+valid_name_1+'.txt')\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1266.9010841846466\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "start=time.time()\n",
    "original_gbm_ffm_data(x_ffm_test,y_test,dict_gbm,x_test_leaves,save_name=r'/home/jupyter/xlearn/gbm_orig_ffm/'+'ffm_test.txt')\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 FFM模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 原始数据的ffm模型初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original Training task\n",
    "ffm_model = xl.create_ffm() # Use field-aware factorization machine\n",
    "ffm_model.setTrain(r'/home/jupyter/xlearn/gbm_orig_ffm/ffm_'+train_name_1+'.txt')  # Training data\n",
    "ffm_model.setValidate(r'/home/jupyter/xlearn/gbm_orig_ffm/ffm_'+valid_name_1+'.txt')  # Validation data\n",
    "#ffm_model.setTest(r'/home/jupyter/xlearn/gbm_orig_ffm/ffm_'+valid_name_1+'.txt')  # test data\n",
    "ffm_model.setTest(r'/home/jupyter/xlearn/gbm_orig_ffm/'+'ffm_test.txt')  # test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313.62089800834656\n"
     ]
    }
   ],
   "source": [
    "# param:\n",
    "#  0. binary classification\n",
    "#  1. learning rate: 0.2\n",
    "#  2. regular lambda: 0.002\n",
    "#  3. evaluation metric: accuracy\n",
    "start=time.time()\n",
    "param = {'task':'binary','k':4,'opt':'adagrad','lr':0.2,'lambda':0.0002, \n",
    "         'metric':'acc','fold':2,'epoch':40,'stop_window':3}\n",
    "#ffm_model.setSign() # Convert output to 0-1  scope\n",
    "ffm_model.setSigmoid()\n",
    "result=ffm_model.fit(param, r'/home/jupyter/xlearn/gbm_orig_ffm/model.out')\n",
    "result_cv=ffm_model.cv(param)\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss of test : 0.4097700772907551\n"
     ]
    }
   ],
   "source": [
    "# Prediction task\n",
    "time.sleep(5)\n",
    "ffm_model.predict(r'/home/jupyter/xlearn/gbm_orig_ffm/model.out', r'/home/jupyter/xlearn/gbm_orig_ffm/output.txt')\n",
    "\n",
    "y_ffm_predict=pd.read_csv(r'/home/jupyter/xlearn/gbm_orig_ffm/output.txt',header=None)\n",
    "logloss = log_loss(y_test, y_ffm_predict)\n",
    "print (\"logloss of test :\",logloss )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FFM模型参数调整-调整隐层K，K还是取4吧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391.52443170547485\n",
      "logloss of test : 0.41501459668314483\n"
     ]
    }
   ],
   "source": [
    "# param:\n",
    "#  0. binary classification\n",
    "#  1. learning rate: 0.2\n",
    "#  2. regular lambda: 0.002\n",
    "#  3. evaluation metric: accuracy\n",
    "start=time.time()\n",
    "param = {'task':'binary','k':8,'opt':'adagrad','lr':0.2,'lambda':0.0002, \n",
    "         'metric':'acc','fold':2,'epoch':40,'stop_window':3}\n",
    "#ffm_model.setSign() # Convert output to 0-1  scope\n",
    "ffm_model.setSigmoid()\n",
    "result=ffm_model.fit(param, r'/home/jupyter/xlearn/gbm_orig_ffm/model.out')\n",
    "result_cv=ffm_model.cv(param)\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "\n",
    "# Prediction task\n",
    "time.sleep(5)\n",
    "ffm_model.predict(r'/home/jupyter/xlearn/gbm_orig_ffm/model.out', r'/home/jupyter/xlearn/gbm_orig_ffm/output.txt')\n",
    "y_ffm_predict=pd.read_csv(r'/home/jupyter/xlearn/gbm_orig_ffm/output.txt',header=None)\n",
    "logloss = log_loss(y_test, y_ffm_predict)\n",
    "print (\"logloss of test :\",logloss )\n",
    "#0.4001,0.4091,0.415"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### lambda调整，0.0001,0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374.90474033355713\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-189-f54a89729bcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mffm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'/home/jupyter/xlearn/gbm_orig_ffm/model.out'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mr'/home/jupyter/xlearn/gbm_orig_ffm/output.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0my_ffm_predict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'/home/jupyter/xlearn/gbm_orig_ffm/output.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mlogloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ffm_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"logloss of test :\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogloss\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mlog_loss\u001b[0;34m(y_true, y_pred, eps, normalize, sample_weight, labels)\u001b[0m\n\u001b[1;32m   1638\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mlogarithm\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnatural\u001b[0m \u001b[0mlogarithm\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1639\u001b[0m     \"\"\"\n\u001b[0;32m-> 1640\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1641\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 44\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# param:\n",
    "#  0. binary classification\n",
    "#  1. learning rate: 0.2\n",
    "#  2. regular lambda: 0.002\n",
    "#  3. evaluation metric: accuracy\n",
    "start=time.time()\n",
    "param = {'task':'binary','k':8,'opt':'adagrad','lr':0.2,'lambda':0.0001, \n",
    "         'metric':'acc','fold':2,'epoch':40,'stop_window':3}\n",
    "#ffm_model.setSign() # Convert output to 0-1  scope\n",
    "ffm_model.setSigmoid()\n",
    "result=ffm_model.fit(param, r'/home/jupyter/xlearn/gbm_orig_ffm/model.out')\n",
    "result_cv=ffm_model.cv(param)\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "\n",
    "# Prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss of test : 0.4089657915496744\n"
     ]
    }
   ],
   "source": [
    "time.sleep(5)\n",
    "ffm_model.predict(r'/home/jupyter/xlearn/gbm_orig_ffm/model.out', r'/home/jupyter/xlearn/gbm_orig_ffm/output.txt')\n",
    "y_ffm_predict=pd.read_csv(r'/home/jupyter/xlearn/gbm_orig_ffm/output.txt',header=None)\n",
    "logloss = log_loss(y_test, y_ffm_predict)\n",
    "print (\"logloss of test :\",logloss )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248.4234766960144\n"
     ]
    }
   ],
   "source": [
    "# param:\n",
    "#  0. binary classification\n",
    "#  1. learning rate: 0.2\n",
    "#  2. regular lambda: 0.002\n",
    "#  3. evaluation metric: accuracy\n",
    "start=time.time()\n",
    "param = {'task':'binary','k':4,'opt':'adagrad','lr':0.2,'lambda':0.000075, \n",
    "         'metric':'acc','fold':2,'epoch':40,'stop_window':3}\n",
    "#ffm_model.setSign() # Convert output to 0-1  scope\n",
    "ffm_model.setSigmoid()\n",
    "result=ffm_model.fit(param, r'/home/jupyter/xlearn/gbm_orig_ffm/model.out')\n",
    "result_cv=ffm_model.cv(param)\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss of test : 0.4075080439133808\n"
     ]
    }
   ],
   "source": [
    "time.sleep(5)\n",
    "ffm_model.predict(r'/home/jupyter/xlearn/gbm_orig_ffm/model.out', r'/home/jupyter/xlearn/gbm_orig_ffm/output.txt')\n",
    "y_ffm_predict=pd.read_csv(r'/home/jupyter/xlearn/gbm_orig_ffm/output.txt',header=None)\n",
    "logloss = log_loss(y_test, y_ffm_predict)\n",
    "print (\"logloss of test :\",logloss )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154.01739239692688\n"
     ]
    }
   ],
   "source": [
    "# param:\n",
    "#  0. binary classification\n",
    "#  1. learning rate: 0.2\n",
    "#  2. regular lambda: 0.002\n",
    "#  3. evaluation metric: accuracy\n",
    "start=time.time()\n",
    "param = {'task':'binary','k':4,'opt':'adagrad','lr':0.2,'lambda':0.00005, \n",
    "         'metric':'acc','fold':2,'epoch':21,'stop_window':3}\n",
    "#ffm_model.setSign() # Convert output to 0-1  scope\n",
    "ffm_model.setSigmoid()\n",
    "result=ffm_model.fit(param, r'/home/jupyter/xlearn/gbm_orig_ffm/model.out')\n",
    "result_cv=ffm_model.cv(param)\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss of test : 0.410874928466404\n"
     ]
    }
   ],
   "source": [
    "time.sleep(5)\n",
    "ffm_model.predict(r'/home/jupyter/xlearn/gbm_orig_ffm/model.out', r'/home/jupyter/xlearn/gbm_orig_ffm/output.txt')\n",
    "y_ffm_predict=pd.read_csv(r'/home/jupyter/xlearn/gbm_orig_ffm/output.txt',header=None)\n",
    "logloss = log_loss(y_test, y_ffm_predict)\n",
    "print (\"logloss of test :\",logloss )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
